---
title: "Classification Methods"
author: "Pratik H"
date: "2023-03-25"
output: pdf_document
---

```{r}
library(ISLR)
```

```{r}
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
```

```{r}
cor(Smarket[,-9])
```

```{r}
attach(Smarket)
plot(Volume)
```

**Logistic Regression**
```{r}
glm.fits = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data = Smarket, family = binomial)
summary(glm.fits)
```

```{r}
coef(glm.fits)
summary(glm.fits)$coef
summary(glm.fits)$coef[,4]
```

```{r}
glm.probs = predict(glm.fits,type="response")
glm.probs[1:10]
contrasts(Direction)
```
```{r}
glm.pred = rep("Down",1250)
glm.pred[glm.probs>.5] = "Up"
```

```{r}
table(glm.pred,Direction)
mean(glm.pred == Direction)
```
The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. 
Dividing the dataset into test and train for better predicting model accuracy.

```{r}
train = (Year<2005)
Smarket.2005 = Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
```

```{r}
glm.fits = glm(Direction~Lag1 +Lag2 + Lag3 + Lag4 + Lag5 + Volume,data = Smarket,family = binomial,subset = train)
glm.probs = predict(glm.fits,Smarket.2005,type = "response")
```

```{r}
glm.pred = rep("Down",252)
glm.pred[glm.probs>0.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005) #training set error rate
mean(glm.pred!=Direction.2005)

```
there is not much drastic change, the test error is still high. 
to fix this, we can try to create a new model taking only features that have the lowest p-value, i.e Lag1 and Lag2
```{r}
glm.fits=glm(Direction~Lag1+Lag2,data = Smarket,family = binomial,subset = train)
glm.probs = predict(glm.fits,Smarket.2005,type="response")
glm.pred = rep("Down",252)
glm.pred[glm.probs>0.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)

```
56% correct, that's not that great.

Let's try predicting in new values of Lag1 and Lag2 (1,2,1.5) and (1.1,-0.8).
```{r}
predict(glm.fits,newdata = data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
```



**Linear Discriminant Analysis**

```{r}
library(MASS)
```
```{r}
lda.fit = lda(Direction~Lag1+Lag2,data=Smarket,subset = train)
lda.fit
# The LDA output indicates that πˆ1 = 0.492 and πˆ2 = 0.508; in other words, 49.2% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of μk. These suggest that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in (4.19). If −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline. 
```
```{r}
plot(lda.fit) # The plot() function produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations.
```
```{r}
lda.pred = predict(lda.fit,Smarket.2005)
names(lda.pred)
```

```{r}
lda.class=lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class == Direction.2005)
```
Applying a 50 % threshold to the posterior probabilities allows us to recre- ate the predictions contained in lda.pred$class.

```{r}
sum(lda.pred$posterior[,1]>=0.5)
sum(lda.pred$posterior[,1]<0.5)
```
Notice that the posterior probability output by the model corresponds to the probability that the market will decrease:
```{r}
lda.pred$posterior[1:20,1] 
lda.class[1:20]
```
**Quadratic Discriminant Analysis**

```{r}
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket ,subset=train)
qda.fit
```
It does not have coefficents because QDA does not have linear function of predictors, rather it has Quadratic function. 

```{r}
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class ,Direction.2005)
mean(qda.class==Direction.2005)
```
Interestingly, the QDA predictions are accurate almost 60% of the time, even though the 2005 data was not used to fit the model. This level of accu- racy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.


**K-Nearest Neighbors**
KNN model requires 4 inputs,
1. A matrix containing the predictors associated with the training data, labeled train.X below.
2. A matrix containing the predictors associated with the data for which we wish to make predictions, labeled test.X below.
3. A vector containing the class labels for the training observations, labeled train.Direction below.
4. A value for K, the number of nearest neighbors to be used by the classifier.

```{r}
library(class)
```

```{r}
train.X = cbind(Lag1,Lag2)[train,]
test.X = cbind(Lag1,Lag2)[!train,]
train.Direction = Direction[train]
```
We set a random seed before we apply knn() because if several observations are tied as nearest neighbors, then R will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.

```{r}
set.seed(1)
knn.pred = knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
mean(knn.pred == Direction.2005)
```

```{r}
knn.pred = knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
mean(knn.pred == Direction.2005)
```


*An Application to Carvan Insurance Data*

```{r}
dim(Caravan)
attach(Caravan)
summary(Purchase)
348 / (348+5474)
```
We can see that there are only 6% of observations which take carvan insurance.

For KNN, it is necessary to standardise the data since after standardisation, the scale for features will be in same range.
```{r}
standardized.X = scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
```
Now every column of standardized.X has a standard deviation of one and a mean of zero.

Splitting the data into test and train 

```{r}
test = 1:1000
train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = Purchase[-test]
test.Y = Purchase[test]
set.seed(1)
knn.pred =knn(train.X,test.X,train.Y,k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
```

```{r}
table(knn.pred,test.Y)
```

```{r}
knn.pred=knn(train.X,test.X,train.Y,k=3)
table(knn.pred,test.Y)
5/26
```
```{r}
knn.pred=knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
4/15
```















